{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Quirkshop_2020_TFF_Intro_Tutorial_User_Copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fra1977/TF_FederatedLearning_Workshop/blob/dev/Quirkshop_2020_TFF_Intro_Tutorial_User_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qN8P0AnTnAhh"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MnUwFbCAKB2r"
      },
      "source": [
        "## Before we start\n",
        "\n",
        "Before we start, please run the following to make sure that your environment is\n",
        "correctly setup. If you don't see a greeting, please refer to the\n",
        "[Installation](../install.md) guide for instructions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZrGitA_KnRO0",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "3e64452f-aa9e-416a-a4d7-a7e99dda2db2"
      },
      "source": [
        "#@title Upgrade tensorflow_federated and load TensorBoard\n",
        "#@test {\"skip\": true}\n",
        "!pip install --quiet --upgrade tensorflow_federated\n",
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import sys\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 481kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 41.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 41.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 44.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 25.6MB/s \n",
            "\u001b[?25hCollecting nest_asyncio\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/44/f2983c5be9803b08f89380229997e92c4bdd7a4a510ccee565b599d1bdc8/nest_asyncio-1.4.0-py3-none-any.whl\n",
            "Installing collected packages: nest-asyncio\n",
            "Successfully installed nest-asyncio-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8BKyHkMxKHfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "48c3f162-04a2-46d1-84af-df44cb8d8202"
      },
      "source": [
        "import collections\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display, HTML, IFrame\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from skimage import io\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def greetings():\n",
        "  display(HTML('<b><font size=\"6\" color=\"#ff00f4\">Greetings, virtual tutorial participants!</font></b>'))\n",
        "  return True\n",
        "l = tff.federated_computation(greetings)()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<b><font size=\"6\" color=\"#ff00f4\">Greetings, virtual tutorial participants!</font></b>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AftvNA5VMemJ"
      },
      "source": [
        "# Federated Learning for Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "coAumH42q9nz"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.14.0/docs/tutorials/federated_learning_for_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.14.0/docs/tutorials/federated_learning_for_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zs2LgZBOMt4M"
      },
      "source": [
        "Let's experiment with federated learning in simulation. In this tutorial, we use the classic MNIST training example to introduce the\n",
        "Federated Learning (FL) API layer of TFF, `tff.learning` - a set of\n",
        "higher-level interfaces that can be used to perform common types of federated\n",
        "learning tasks, such as federated training, against user-supplied models\n",
        "implemented in TensorFlow.\n",
        "\n",
        "<center><img src=https://storage.googleapis.com/tfds-data/visualization/mnist-3.0.1.png width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN7n8RS7-rLR",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial Outline\n",
        "We'll be training a model to perform image classification using the classic MNIST dataset, with the neural net learning to classify digit from image. In this case, we'll be simulating federated learning, with the training data distributed on different devices.\n",
        "\n",
        "<p><b>Sections</b></p>\n",
        "\n",
        "\n",
        "1.   Load TFF Libraries.\n",
        "2.   Explore/preprocess federated EMNIST dataset.\n",
        "3.   Create a model.\n",
        "4.   Set up federated averaging process for training.\n",
        "5.   Analyze training metrics.\n",
        "6.   Set up federated evaluation computation.\n",
        "7.   Analyze evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Cyy2AWbLMKj"
      },
      "source": [
        "## Preparing the input data\n",
        "\n",
        "Let's start with the data. Federated learning requires a federated data set,\n",
        "i.e., a collection of data from multiple users. Federated data is typically\n",
        "non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables),\n",
        "which poses a unique set of challenges. Users typically have different distributions of data depending on usage patterns.\n",
        "\n",
        "In order to facilitate experimentation, we seeded the TFF repository with a few\n",
        "datasets.\n",
        "\n",
        "Here's how we can load our sample dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NayDhCX6SjwE",
        "colab": {}
      },
      "source": [
        "# Your code here for loading the emnist dataset from the TFF repository.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yeX8BKgPfeFw"
      },
      "source": [
        "The data sets returned by `load_data()` are instances of\n",
        "`tff.simulation.ClientData`, an interface that allows you to enumerate the set\n",
        "of users, to construct a `tf.data.Dataset` that represents the data of a\n",
        "particular user, and to query the structure of individual elements.\n",
        "\n",
        "Let's explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kN4-U5nJgKig",
        "colab": {}
      },
      "source": [
        "len(emnist_train.client_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZyCzIrSegT62",
        "colab": {}
      },
      "source": [
        "# Let's look at the shape of our data\n",
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0])\n",
        "\n",
        "example_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EsvSXGEMgd9G",
        "colab": {}
      },
      "source": [
        "# Let's select an example dataset from one of our simulated clients\n",
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0])\n",
        "\n",
        "# Your code to get an example element from one client:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmLV0nfMg98V",
        "colab": {}
      },
      "source": [
        "plt.imshow(example_element['pixels'].numpy(), cmap='gray', aspect='equal')\n",
        "plt.grid(False)\n",
        "_ = plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3KUf0kC8TN",
        "colab_type": "text"
      },
      "source": [
        "**Exploring non-iid data**\n",
        "\n",
        "Use this time to ask questions on the dory if stuck / get caught up.\n",
        "This section is separate from the rest of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veWNAxdQfrgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Example MNIST digits for one client\n",
        "f = plt.figure(figsize=(20,4))\n",
        "j = 0\n",
        "\n",
        "for e in example_dataset.take(40):\n",
        "  plt.subplot(4, 10, j+1)\n",
        "  plt.imshow(e['pixels'].numpy(), cmap='gray', aspect='equal')\n",
        "  plt.axis('off')\n",
        "  j += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XJRWDFWniik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of examples per layer for a sample of clients\n",
        "f = plt.figure(figsize=(12,7))\n",
        "f.suptitle(\"Label Counts for a Sample of Clients\")\n",
        "for i in range(6):\n",
        "  ds = emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[i])\n",
        "  k = collections.defaultdict(list)\n",
        "  for e in ds:\n",
        "    k[e['label'].numpy()].append(e['label'].numpy())\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.title(\"Client {}\".format(i))\n",
        "  for j in range(10):\n",
        "    plt.hist(k[j], density=False, bins=[0,1,2,3,4,5,6,7,8,9,10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOUI4zW9LQNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's play around with the emnist_train dataset.\n",
        "# Let's explore the non-iid charateristic of the example data.\n",
        "\n",
        "for i in range(5):\n",
        "  ds = emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[i])\n",
        "  k = collections.defaultdict(list)\n",
        "  for e in ds:\n",
        "    k[e['label'].numpy()].append(e['pixels'].numpy())\n",
        "  f = plt.figure(i, figsize=(12,5))\n",
        "  f.suptitle(\"Client #{}'s Mean Image Per Label\".format(i))\n",
        "  for j in range(10):\n",
        "    mn_img = np.mean(k[j],0)\n",
        "    plt.subplot(2, 5, j+1)\n",
        "    plt.imshow(mn_img.reshape((28,28)))#,cmap='gray') \n",
        "    plt.axis('off')\n",
        "\n",
        "# Each client has different mean images -- each client will be nudging the model\n",
        "# in their own directions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lMd01egqy9we"
      },
      "source": [
        "### Preprocessing the data\n",
        "\n",
        "Since the data is already a `tf.data.Dataset`,  preprocessing can be accomplished using Dataset transformations. [See here](https://www.tensorflow.org/guide/data) for more detail on these transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cyG_BMraSuu_",
        "colab": {}
      },
      "source": [
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER=10\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['pixels'], [-1, 784]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m9LXykN_jlJw"
      },
      "source": [
        "Let's verify this worked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VChB7LMQjkYz",
        "colab": {}
      },
      "source": [
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JGsMvRQt9Agl"
      },
      "source": [
        "Here's a simple helper function that will construct a list of datasets from the\n",
        "given set of users as an input to a round of training or evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_PHMvHAI9xVc",
        "colab": {}
      },
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0M9PfjOtAVqw"
      },
      "source": [
        "Now, how do we choose clients?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GZ6NYHxB8xer",
        "colab": {}
      },
      "source": [
        "sample_clients = emnist_train.client_ids[0:NUM_CLIENTS]\n",
        "\n",
        "# Your code to get the federated dataset here for the sampled clients:\n",
        "\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HOxq4tbi9m8-"
      },
      "source": [
        "## Creating a model with Keras\n",
        "\n",
        "If you are using Keras, you likely already have code that constructs a Keras\n",
        "model. Here's an example of a simple model that will suffice for our needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LYCsJGJFWbqt",
        "colab": {}
      },
      "source": [
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Input(shape=(784,)),\n",
        "      tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "      tf.keras.layers.Softmax(),\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0214iKjCTyX",
        "colab_type": "text"
      },
      "source": [
        "**Centralized training with Keras**\n",
        "\n",
        "Use this time to ask questions on the dory if stuck / get caught up.\n",
        "This section is separate from the rest of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5XW_p4iLlJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Centralized training with keras ---------------------------------------------\n",
        "\n",
        "# This is separate from the TFF tutorial, and demonstrates how to train a\n",
        "# Keras model in a centralized fashion (contrasting training in a federated env)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data (these are NumPy arrays)\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "\n",
        "y_train = y_train.astype(\"float32\")\n",
        "\n",
        "mod = create_keras_model()\n",
        "mod.compile(\n",
        "    optimizer=tf.keras.optimizers.RMSprop(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "h = mod.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=2\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NHdraKFH4OU2"
      },
      "source": [
        "\n",
        "\n",
        "In order to use any model with TFF, it needs to be wrapped in an instance of the\n",
        "`tff.learning.Model` interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA8cJoGE3Rh_",
        "colab_type": "text"
      },
      "source": [
        "More keras metrics you can add are [found here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q3ynrxd53HzY",
        "colab": {}
      },
      "source": [
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XJ5E3O18_JZ6"
      },
      "source": [
        "## Training the model on federated data\n",
        "\n",
        "Now that we have a model wrapped as `tff.learning.Model` for use with TFF, we\n",
        "can let TFF construct a Federated Averaging algorithm by invoking the helper\n",
        "function `tff.learning.build_federated_averaging_process`, as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sk6mjOfycX5N",
        "colab": {}
      },
      "source": [
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    # Add server optimizer here!\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f8FpvN2n67sm"
      },
      "source": [
        "What just happened? TFF has constructed a pair of *federated computations* and\n",
        "packaged them into a `tff.templates.IterativeProcess` in which these computations\n",
        "are available as a pair of properties `initialize` and `next`.\n",
        "\n",
        "An iterative process will usually be driven by a control loop like:\n",
        "```\n",
        "def initialize():\n",
        "  ...\n",
        "\n",
        "def next(state):\n",
        "  ...\n",
        "\n",
        "iterative_process = IterativeProcess(initialize, next)\n",
        "state = iterative_process.initialize()\n",
        "for round in range(num_rounds):\n",
        "  state = iterative_process.next(state)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v1gbHQ_7BiyT"
      },
      "source": [
        "Let's invoke the `initialize` computation to construct the server state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6cagCWlZmcch",
        "colab": {}
      },
      "source": [
        "state = iterative_process.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TjjxTx9e_rMd"
      },
      "source": [
        "The second of the pair of federated computations, `next`, represents a single\n",
        "round of Federated Averaging, which consists of pushing the server state\n",
        "(including the model parameters) to the clients, on-device training on their\n",
        "local data, collecting and averaging model updates, and producing a new updated\n",
        "model at the server.\n",
        "\n",
        "Let's run a single round of training and visualize the results. We can use the\n",
        "federated data we've already generated above for a sample of users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F3M_W9dDE6Tm",
        "colab": {}
      },
      "source": [
        "# Run one single round of training.\n",
        "state, metrics = iterative_process.next(state, federated_train_data)\n",
        "print('round  1, metrics={}'.format(metrics['train']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UmhReXt9G4A5"
      },
      "source": [
        "Let's run a few more rounds. As noted earlier, typically at this point you would\n",
        "pick a subset of your simulation data from a new randomly selected sample of\n",
        "users for each round in order to simulate a realistic deployment in which users\n",
        "continuously come and go, but in this interactive notebook, for the sake of\n",
        "demonstration we'll just reuse the same users, so that the system converges\n",
        "quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qrJkQuCRJP9C",
        "colab": {}
      },
      "source": [
        "NUM_ROUNDS = 11\n",
        "for round_num in range(2, NUM_ROUNDS):\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics['train']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "joHYzn9jcs0Y"
      },
      "source": [
        "Training loss is decreasing after each round of federated training, indicating\n",
        "the model is converging. There are some important caveats with these training\n",
        "metrics, however, see the section on *Evaluation* later in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ruSHJl1IjhNf"
      },
      "source": [
        "##Displaying model metrics in TensorBoard\n",
        "Next, let's visualize the metrics from these federated computations using Tensorboard.\n",
        "\n",
        "Let's start by creating the directory and the corresponding summary writer to write the metrics to.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3QUBK41lWDW",
        "colab": {}
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "logdir = \"/tmp/logs/scalars/training/\"\n",
        "if os.path.exists(logdir):\n",
        "  shutil.rmtree(logdir)\n",
        "\n",
        "# Your code to create a summary writer:\n",
        "\n",
        "state = iterative_process.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w-2aGxUlzS_J"
      },
      "source": [
        "Plot the relevant scalar metrics with the same summary writer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JZtr4_8lzN-V",
        "colab": {}
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "with summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS):\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    for name, value in metrics['train'].items():\n",
        "      tf.summary.scalar(name, value, step=round_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iUouyAHG0Mk8"
      },
      "source": [
        "Start TensorBoard with the root log directory specified above. It can take a few seconds for the data to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urYYcmA9089p",
        "colab": {}
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "%tensorboard --logdir /tmp/logs/scalars/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jejrFEVP1EDs"
      },
      "source": [
        "In order to view evaluation metrics the same way, you can create a separate eval folder, like \"logs/scalars/eval\", to write to TensorBoard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m7lz59lMJ0kj"
      },
      "source": [
        "## Evaluation\n",
        "To perform evaluation on federated data, you can construct another *federated\n",
        "computation* designed for just this purpose, using the\n",
        "`tff.learning.build_federated_evaluation` function, and passing in your model\n",
        "constructor as an argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nRiXyqnXM2VO",
        "colab": {}
      },
      "source": [
        "# Construct federated evaluation computation here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SpfgdNDoRjPy"
      },
      "source": [
        "Now, let's compile a test sample of federated data and rerun evaluation on the\n",
        "test data. The data will come from a different sample of users, but from a\n",
        "distinct held-out data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "in8vProVNc04",
        "colab": {}
      },
      "source": [
        "import random\n",
        "shuffled_ids = emnist_test.client_ids.copy()\n",
        "random.shuffle(shuffled_ids)\n",
        "sample_clients = shuffled_ids[0:NUM_CLIENTS]\n",
        "\n",
        "federated_test_data = make_federated_data(emnist_test, sample_clients)\n",
        "\n",
        "len(federated_test_data), federated_test_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ty-ZwfE0NJfV",
        "colab": {}
      },
      "source": [
        "# Run evaluation on the test data here, using the federated model produced from \n",
        "# training:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e5fGtIJYNqYH",
        "colab": {}
      },
      "source": [
        "str(test_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "67vYxrDWzRcj"
      },
      "source": [
        "This concludes the tutorial. We encourage you to play with the\n",
        "parameters (e.g., batch sizes, number of users, epochs, learning rates, etc.), to modify the code above to simulate training on random samples of users in\n",
        "each round, and to explore the other tutorials we've developed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qgNVSGKAgRwy"
      },
      "source": [
        "# Federated Learning for Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BLbb4ALNgRw5"
      },
      "source": [
        "This tutorial builds on the concepts in the [Federated Learning for Image Classification](federated_learning_for_image_classification.ipynb) tutorial, and demonstrates several other useful approaches for federated learning.\n",
        "\n",
        "In particular, we load a previously trained Keras model, and refine it using  federated training on a (simulated) decentralized dataset. This is practically important for several reasons. The ability to use serialized models makes it easy to mix federated learning with other ML approaches. Further, this allows use of an increasing range of pre-trained models --- for example, training language models from scratch is rarely necessary, as numerous pre-trained models are now widely available (see, e.g., [TF Hub](https://www.tensorflow.org/hub)). Instead, it makes more sense to start from a pre-trained model, and refine it using Federated Learning, adapting to the particular characteristics of the decentralized data for a particular application.\n",
        "\n",
        "We will also demonstrate creating your own TFF dataset from a CSV file, so that you can begin using TFF on your own datasets.\n",
        "\n",
        "For this tutorial, we start with a RNN that generates ASCII characters, and refine it via federated learning. We also show how the final weights can be fed back to the original Keras model, allowing easy evaluation and text generation using standard tools.\n",
        "\n",
        "<p><b>Sections</b></p>\n",
        "\n",
        "1.   Load and test an existing text generation model.\n",
        "2.   Create a TFF dataset from a CSV file of Shakespeare data.\n",
        "3.   Preprocess the data and test the model on the new data.\n",
        "4.   Simulate cross-silo federated learning with centralized evaluation.\n",
        "5.   Simulate cross-device federated learning with centralized evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6lNk2aggRw6"
      },
      "source": [
        "## Load a pre-trained model\n",
        "\n",
        "We load a model that was pre-trained following the TensorFlow tutorial\n",
        "[Text generation using a RNN with eager execution](https://www.tensorflow.org/tutorials/sequences/text_generation). However,\n",
        "rather than training on [The Complete Works of Shakespeare](http://www.gutenberg.org/files/100/100-0.txt), we pre-trained the model on the text from the Charles Dickens'\n",
        "    [A Tale of Two Cities](http://www.ibiblio.org/pub/docs/books/gutenberg/9/98/98.txt)\n",
        "    and\n",
        "    [A Christmas Carol](http://www.ibiblio.org/pub/docs/books/gutenberg/4/46/46.txt).\n",
        " \n",
        "Other than expanding the vocabulary, we didn't modify the original tutorial, so this initial model isn't state-of-the-art, but it produces reasonable predictions and is sufficient for our tutorial purposes. The final model was saved with `tf.keras.models.save_model(include_optimizer=False)`.\n",
        "   \n",
        " We will use federated learning to fine-tune this model for Shakespeare in this tutorial, using a federated version of the data provided by TFF.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYF75lChgRw7",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def load_model(batch_size):\n",
        "  urls = {\n",
        "      1: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel',\n",
        "      8: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel'}\n",
        "  assert batch_size in urls, 'batch_size must be in ' + str(urls.keys())\n",
        "  url = urls[batch_size]\n",
        "  local_file = tf.keras.utils.get_file(os.path.basename(url), origin=url)  \n",
        "  return tf.keras.models.load_model(local_file, compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zZvAogCmgRxB"
      },
      "source": [
        "### Test the loaded model\n",
        "Let's create some vocab lookup tables in order to generate some text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YoajrAgmgRxC",
        "colab": {}
      },
      "source": [
        "# A fixed vocabulary of ASCII chars that occur in the works of Shakespeare and Dickens:\n",
        "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_mGAUgLDgRxE",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # From https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "  num_generate = 200\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(\n",
        "        predictions, num_samples=1)[-1, 0].numpy()\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlz6gmargRxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "841a0fd5-675c-447d-cf2f-40b07156f5d3"
      },
      "source": [
        "# Text generation requires a batch_size=1 model.\n",
        "keras_model_batch1 = load_model(batch_size=1)\n",
        "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What of TensorFlow Federated, you ask? The honour\r\n",
            "of a daughter of Business, there was yet so aristocrat, an enjoying the\r\n",
            "brill again.\r\n",
            "\r\n",
            "The faces consisted.\r\n",
            "\r\n",
            "\"Is there no good that was a profounder room, they are still together; he w\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P7sSr-HygRxK"
      },
      "source": [
        "## Load the Federated Shakespeare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-KFMIP1NgRxL"
      },
      "source": [
        "The tff.simulation.datasets package provides a variety of datasets that are split into \"clients\", where each client corresponds to a dataset on a particular device that might participate in federated learning.\n",
        "\n",
        "These datasets provide realistic non-IID data distributions that replicate in simulation the challenges of training on real decentralized data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uI_emtkEgRxL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "241c1b39-e438-4a37-b250-d74429f5e6e4"
      },
      "source": [
        "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/shakespeare.tar.bz2\n",
            "1851392/1848122 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vB8mewVNgRxP"
      },
      "source": [
        "You may want to use your own dataset with TFF. To demonstrate loading a CSV file into TFF, let's write the data from the Shakespeare datasets to CSV and download the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TZh5717agRxQ",
        "colab": {}
      },
      "source": [
        "def write_data_to_csv_file(tff_dataset, f):\n",
        "  f.write('\"character\",\"snippets\"\\n')\n",
        "  for client_id in tff_dataset.client_ids:\n",
        "    tf_dataset = tff_dataset.create_tf_dataset_for_client(client_id)\n",
        "    for element in tf_dataset.as_numpy_iterator():\n",
        "      # The CSV standard specifies that double quotes in the data must be\n",
        "      # escaped by preceding them with another double quote.\n",
        "      f.write('\"' + client_id + '\",\"' + str(element['snippets'], 'ascii')\n",
        "        .replace('\"', '\"\"') + '\"\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jQi7gAmRgRxS",
        "colab": {}
      },
      "source": [
        "filenames = ['shakespeare_train.csv', 'shakespeare_test.csv']\n",
        "for filename, data in zip(filenames, [train_data, test_data]):\n",
        "  with open(filename, 'w') as f:\n",
        "    write_data_to_csv_file(data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFChL3PYgRxU"
      },
      "source": [
        "Let's see what the first few lines of each file look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oigmdRB-gRxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "abb1d4fd-fc19-45ce-9d89-2e55d4c036ec"
      },
      "source": [
        "for filename in filenames:\n",
        "  with open(filename, 'r') as f:\n",
        "    print(\"~~~~~~~~~Reading file \" + filename + \"~~~~~~~~~\")\n",
        "    for i in range(10):\n",
        "      print(f.readline())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~Reading file shakespeare_train.csv~~~~~~~~~\n",
            "\"character\",\"snippets\"\n",
            "\n",
            "\"ALL_S_WELL_THAT_ENDS_WELL_ADAM\",\"Yonder comes my master, your brother.\"\n",
            "\n",
            "\"ALL_S_WELL_THAT_ENDS_WELL_ADAM\",\"Come not within these doors; within this roof\n",
            "\n",
            "The enemy of all your graces lives.\n",
            "\n",
            "Your brother- no, no brother; yet the son-\n",
            "\n",
            "Yet not the son; I will not call him son\n",
            "\n",
            "Of him I was about to call his father-\n",
            "\n",
            "Hath heard your praises; and this night he means\n",
            "\n",
            "To burn the lodging where you use to lie,\n",
            "\n",
            "And you within it. If he fail of that,\n",
            "\n",
            "~~~~~~~~~Reading file shakespeare_test.csv~~~~~~~~~\n",
            "\"character\",\"snippets\"\n",
            "\n",
            "\"ALL_S_WELL_THAT_ENDS_WELL_ADAM\",\"I down, and measure out my grave. Farewell, kind master.\n",
            "\n",
            "So had you need;\"\n",
            "\n",
            "\"ALL_S_WELL_THAT_ENDS_WELL_ADAM\",\"your service. God be with my old master! He would not have spoke\n",
            "\n",
            "such a word.\n",
            "\n",
            "                                     Exeunt ORLANDO and ADAM\n",
            "\n",
            "What, my young master? O my gentle master!\"\n",
            "\n",
            "\"ALL_S_WELL_THAT_ENDS_WELL_AEDILE\",\"I have.\"\n",
            "\n",
            "\"ALL_S_WELL_THAT_ENDS_WELL_AEDILE\",\"I shall inform them.\"\n",
            "\n",
            "\"ALL_S_WELL_THAT_ENDS_WELL_AGRIPPA\",\"There she appear'd indeed! Or my reporter devis'd well for\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YFq25iLngRxX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "dcad61f5-4e42-4846-90bf-ab99c60e98ad"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "for filename in filenames:\n",
        "  files.download(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e59d3410-1522-4318-9cf7-2b8e6f91707e\", \"shakespeare_train.csv\", 3049377)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2b2659a8-6375-41dc-a26b-10bfd6c9c228\", \"shakespeare_test.csv\", 441746)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ADdvUcfygRxZ"
      },
      "source": [
        "Now we can upload the CSV files, as if we had the data in CSV format locally in the first place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zy1GnLHtgRxa",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "3e0b4a27-63e4-445d-bdbc-141c9c68df8c"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cc8a5432-15f9-4066-9f64-66bbcc94375f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cc8a5432-15f9-4066-9f64-66bbcc94375f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving shakespeare_test.csv to shakespeare_test (1).csv\n",
            "Saving shakespeare_train.csv to shakespeare_train (1).csv\n",
            "User uploaded file \"shakespeare_test.csv\" with length 441746 bytes\n",
            "User uploaded file \"shakespeare_train.csv\" with length 3049377 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ET5fTu3tgRxc"
      },
      "source": [
        "We now write a function that returns a TFF dataset given the CSV file contents. We use Pandas to read in the CSV data. Then we implement `create_tf_dataset_for_client_fn` which takes a client ID and returns a TensorFlow dataset for that client. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WT_oKL8VgRxc",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "client_id_colname = 'character' # the column that represents client ID\n",
        "data_colname = 'snippets'\n",
        "\n",
        "def create_tff_dataset_for_csv_file(filename):\n",
        "  # Create a Pandas dataframe from the CSV file.\n",
        "  df = pd.read_csv(io.StringIO(uploaded[filename].decode(\"ascii\")))\n",
        "  # Collect unique character names.\n",
        "  client_ids = df[client_id_colname].unique().tolist()\n",
        "\n",
        "  # Define a function that takes client ID and returns a tf.data.Dataset for\n",
        "  # that client. The tf.data.Dataset should contain a dictionary for each\n",
        "  # line spoken by the character, where the single key in each dictionary is\n",
        "  # \"snippets\" and the value is a string tensor containing the text of the line.\n",
        "  def create_tf_dataset_for_client_fn(client_id):\n",
        "    # Retrieve only the rows corresponding to this client.\n",
        "    client_data = df[df[client_id_colname] == client_id]\n",
        "    # Filter out any rows without a snippet of text spoken by the character.\n",
        "    client_data = client_data[client_data[data_colname].notnull()]\n",
        "    # Select the data columns, discarding the client id column.\n",
        "    client_data = client_data[[data_colname]]\n",
        "    # Convert to a dictionary in the format\n",
        "    # [{column1 : value1, column2 : value2}]\n",
        "    records = client_data.to_dict('records')\n",
        "\n",
        "    # Define a generator that outputs a map for each row with column names as\n",
        "    # keys and row contents as values. In this example there is only one column,\n",
        "    # 'snippets', but this approach is shown to demonstrate how one might\n",
        "    # load a CSV file with more columns.\n",
        "    def dataset_gen():\n",
        "      for row in records:\n",
        "        yield row\n",
        "    # Generate a dataset for the client, specifying the output type explicitly\n",
        "    # as otherwise Tensorflow expects a tensor as the toplevel type.\n",
        "    return tf.data.Dataset.from_generator(dataset_gen,\n",
        "                                          output_types={data_colname: tf.string},\n",
        "                                          output_shapes={data_colname: []}\n",
        "    )\n",
        "\n",
        "  # Your code to call tff.simulation.ClientData.from_clients_and_fn to return a\n",
        "  # TFF dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wlAiDa4SgRxg",
        "colab": {}
      },
      "source": [
        "test_filename = 'shakespeare_test.csv'\n",
        "test_data = create_tff_dataset_for_csv_file(test_filename)\n",
        "train_filename = 'shakespeare_train.csv'\n",
        "train_data = create_tff_dataset_for_csv_file(train_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cOGX5luugRxi"
      },
      "source": [
        "The datasets we just created consist of a sequence of maps from the key 'snippet' to \n",
        "string `Tensors`, one for each line spoken by a particular character in a\n",
        "Shakespeare play. The client keys consist of the name of the play joined with\n",
        "the name of the character, so for example `MUCH_ADO_ABOUT_NOTHING_OTHELLO` corresponds to the lines for the character Othello in the play *Much Ado About Nothing*. Note that in a real federated learning scenario\n",
        "clients are never identified or tracked by ids, but for simulation it is useful\n",
        "to work with keyed datasets.\n",
        "\n",
        "Here, for example, we can look at some data from King Lear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ROj5014ggRxi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9a582f43-bfae-4b12-f72d-29d2fdd11433"
      },
      "source": [
        "# Here the play is \"The Tragedy of King Lear\" and the character is \"King\".\n",
        "raw_example_dataset = train_data.create_tf_dataset_for_client(\n",
        "    'THE_TRAGEDY_OF_KING_LEAR_KING')\n",
        "# To allow for future extensions, each entry x is a dictionary with a single key\n",
        "# 'snippets' which contains the text.\n",
        "for x in raw_example_dataset.take(2):\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'snippets': <tf.Tensor: shape=(), dtype=string, numpy=b\"Live regist'red upon our brazen tombs,\\nAnd then grace us in the disgrace of death;\\nWhen, spite of cormorant devouring Time,\\nTh' endeavour of this present breath may buy\\nThat honour which shall bate his scythe's keen edge,\\nAnd make us heirs of all eternity.\\nTherefore, brave conquerors- for so you are\\nThat war against your own affections\\nAnd the huge army of the world's desires-\\nOur late edict shall strongly stand in force:\\nNavarre shall be the wonder of the world;\\nOur court shall be a little Academe,\\nStill and contemplative in living art.\\nYou three, Berowne, Dumain, and Longaville,\\nHave sworn for three years' term to live with me\\nMy fellow-scholars, and to keep those statutes\\nThat are recorded in this schedule here.\\nYour oaths are pass'd; and now subscribe your names,\\nThat his own hand may strike his honour down\\nThat violates the smallest branch herein.\\nIf you are arm'd to do as sworn to do,\\nSubscribe to your deep oaths, and keep it too.\\nYour oath is pass'd to pass away from these.\">}\n",
            "{'snippets': <tf.Tensor: shape=(), dtype=string, numpy=b'this?\\nDid you hear the proclamation?'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "plA7imzBgRxk"
      },
      "source": [
        "If for any reason the steps above to load from CSV didn't work for you, don't worry- just reload the original datasets from the tff.simulation.datasets package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7m-T0n4gRxl",
        "colab": {}
      },
      "source": [
        "# Only run this cell if the step above did not work.\n",
        "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sOdFAXMygRxn"
      },
      "source": [
        "## Preprocess the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4pc0HZK9gRxn"
      },
      "source": [
        "We now use `tf.data.Dataset` transformations to prepare this data for training the char RNN loaded above. This section doesn't introduce any new TFF concepts,\n",
        "so if you would like to use the time to ask a question on the Dory this would be a good time to take a break.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jAESzUsQgRxo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6a97048-4b95-4083-a352-383b67511989"
      },
      "source": [
        "# Construct a lookup table to map string chars to indexes,\n",
        "# using the vocab loaded above:\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=vocab, values=tf.constant(list(range(len(vocab))),\n",
        "                                       dtype=tf.int64)),\n",
        "    default_value=0)\n",
        "\n",
        "# We can see that the table takes a tensor of characters and produces a tensor \n",
        "# of integer indices.\n",
        "indices = table.lookup(tf.strings.bytes_split(\"Hello World\"))\n",
        "print(indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 8 64  2  2 23 13 31 23 45  2  0], shape=(11,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hEFvHgKZgRxq",
        "colab": {}
      },
      "source": [
        "# Input pre-processing parameters\n",
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 8\n",
        "BUFFER_SIZE = 10000  # For dataset shuffling\n",
        "\n",
        "# Map a single dict containing a snippet to a tensor of int64 indices.\n",
        "def to_ids(x):\n",
        "  s = tf.reshape(x['snippets'], shape=[1])\n",
        "  chars = tf.strings.bytes_split(s).values\n",
        "  ids = table.lookup(chars)\n",
        "  return ids\n",
        "\n",
        "# Given a batch of sequences, split each sequence into the first \n",
        "# SEQ_LENGTH and last SEQ_LENGTH characters. The first SEQ_LENGTH chars\n",
        "# will be the model input and the last SEQ_LENGTH chars will be the model\n",
        "# target.\n",
        "def split_input_target(chunk):\n",
        "  input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
        "  target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
        "  return (input_text, target_text)\n",
        "\n",
        "def preprocess(dataset):\n",
        "  return (\n",
        "      # Map ASCII chars to int64 indexes using the vocab\n",
        "      dataset.map(to_ids)\n",
        "      # Split into individual chars\n",
        "      .unbatch()\n",
        "      # Form example sequences of SEQ_LENGTH +1\n",
        "      .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
        "      # Shuffle and form minibatches\n",
        "      .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "      # And finally split into (input, target) tuples,\n",
        "      # each of length SEQ_LENGTH.\n",
        "      .map(split_input_target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jPN8o3algRxs"
      },
      "source": [
        "Note that in the formation of the original sequences and in the formation of\n",
        "batches above, we use `drop_remainder=True` for simplicity. This means that any\n",
        "characters (clients) that don't have at least `(SEQ_LENGTH + 1) * BATCH_SIZE`\n",
        "chars of text will have empty datasets. A typical approach to address this would\n",
        "be to pad the batches with a special token, and then mask the loss to not take\n",
        "the padding tokens into account.\n",
        "\n",
        "This would complicate the example somewhat, so for this tutorial we only use full batches, as in the\n",
        "[standard tutorial](https://www.tensorflow.org/tutorials/sequences/text_generation).\n",
        "However, in the federated setting this issue is more significant, because many\n",
        "users might have small datasets.\n",
        "\n",
        "Now we can preprocess our `raw_example_dataset`, and check the types:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kv9FH5LEgRxs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab04eb27-af34-47dc-e4b6-42a8344bdb0e"
      },
      "source": [
        "example_dataset = preprocess(raw_example_dataset)\n",
        "print(example_dataset.element_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(8, 100), dtype=tf.int64, name=None), TensorSpec(shape=(8, 100), dtype=tf.int64, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i9Vnc27AgRxw"
      },
      "source": [
        "## Compile the model and test on the preprocessed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MoKoY-JLgRxw"
      },
      "source": [
        "We loaded an uncompiled keras model, but in order to run `keras_model.evaluate`, we need to compile it with a loss and metrics. We will also compile in an optimizer, which will be used as the on-device optimizer in Federated Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H_Ol7OW1gRxx"
      },
      "source": [
        "The original tutorial didn't have char-level accuracy (the fraction\n",
        "of predictions where the highest probability was put on the correct\n",
        "next char). This is a useful metric, so we add it.\n",
        "However, we need to define a new metric class for this because \n",
        "our predictions have rank 3 (a vector of logits for each of the \n",
        "`BATCH_SIZE * SEQ_LENGTH` predictions), and `SparseCategoricalAccuracy`\n",
        "expects only rank 2 predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RNOZU9SegRxx",
        "colab": {}
      },
      "source": [
        "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
        "\n",
        "  def __init__(self, name='accuracy', dtype=tf.float32):\n",
        "    super().__init__(name, dtype=dtype)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.reshape(y_true, [-1, 1])\n",
        "    y_pred = tf.reshape(y_pred, [-1, len(vocab), 1])\n",
        "    return super().update_state(y_true, y_pred, sample_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2HuTGN6NgRxz"
      },
      "source": [
        "Now we can compile a model, and evaluate it on our `example_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0UBWvM92gRxz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d696aa9f-ca1b-415a-952f-05d02e9bec67"
      },
      "source": [
        "BATCH_SIZE = 8  # The training and eval batch size for the rest of this tutorial.\n",
        "keras_model = load_model(batch_size=BATCH_SIZE)\n",
        "keras_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[FlattenedCategoricalAccuracy()])\n",
        "\n",
        "# Confirm that loss is much lower on Shakespeare than on random data\n",
        "loss, accuracy = keras_model.evaluate(example_dataset.take(5), verbose=0)\n",
        "print(\n",
        "    'Evaluating on an example Shakespeare character: {a:3f}'.format(a=accuracy))\n",
        "\n",
        "# As a sanity check, we can construct some completely random data, where we expect\n",
        "# the accuracy to be essentially random:\n",
        "random_guessed_accuracy = 1.0 / len(vocab)\n",
        "print('Expected accuracy for random guessing: {a:.3f}'.format(\n",
        "    a=random_guessed_accuracy))\n",
        "num_steps = 10\n",
        "random_indexes = np.random.randint(\n",
        "    low=0, high=len(vocab), size=1 * BATCH_SIZE * (SEQ_LENGTH + 1) * num_steps)\n",
        "data = collections.OrderedDict(\n",
        "    snippets=tf.constant(\n",
        "        ''.join(np.array(vocab)[random_indexes]), shape=[1, 1]))\n",
        "random_dataset = preprocess(tf.data.Dataset.from_tensor_slices(data))\n",
        "loss, accuracy = keras_model.evaluate(random_dataset, steps=num_steps, verbose=0)\n",
        "print('Evaluating on completely random data: {a:.3f}'.format(a=accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating on an example Shakespeare character: 0.409250\n",
            "Expected accuracy for random guessing: 0.012\n",
            "Evaluating on completely random data: 0.013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckK2wqJ8gRx1"
      },
      "source": [
        "## Fine-tune the model with Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYVhprqBgRx2"
      },
      "source": [
        "TFF serializes all TensorFlow computations so they can potentially be run in a\n",
        "non-Python environment (even though at the moment, only a simulation runtime implemented in Python is available). Even though we are running in eager mode, (TF 2.0), currently TFF serializes TensorFlow computations by constructing the\n",
        "necessary ops inside the context of a \"`with tf.Graph.as_default()`\" statement.\n",
        "Thus, we need to provide a function that TFF can use to introduce our model into\n",
        "a graph it controls. We do this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gBfqiQ8HgRx2",
        "colab": {}
      },
      "source": [
        "# Clone the keras_model inside `create_tff_model()`, which TFF will\n",
        "# call to produce a new copy of the model inside the graph that it will \n",
        "# serialize. Note: we want to construct all the necessary objects we'll need \n",
        "# _inside_ this method.\n",
        "def create_tff_model():\n",
        "  keras_model_clone = tf.keras.models.clone_model(keras_model)\n",
        "\n",
        "  # Your code to create and return a tff.learning.Model using \n",
        "  # tff.learning.from_keras_model, using the same loss function and list of \n",
        "  # metrics we used to compile the Keras model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cWDgNLPFgRx4"
      },
      "source": [
        "Now we are ready to construct a Federated Averaging iterative process, which we will use to improve the model (for details on the Federated Averaging algorithm, see the paper [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c9J7LPPfgRx5",
        "colab": {}
      },
      "source": [
        "# This command builds all the TensorFlow graphs and serializes them: \n",
        "fed_avg = tff.learning.build_federated_averaging_process(\n",
        "    model_fn=create_tff_model,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JjR0IVqzgRx8"
      },
      "source": [
        "The initial state of the model produced by `fed_avg.initialize()` is based\n",
        "on the random initializers for the Keras model, not the weights that were loaded,\n",
        "since `clone_model()` does not clone the weights. To start training\n",
        "from a pre-trained model, we set the model weights in the server state\n",
        "directly from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MGkv7c-PgRx9",
        "colab": {}
      },
      "source": [
        "def init_state():\n",
        "  # The state of the FL server, containing the model and optimization state.\n",
        "  state = fed_avg.initialize()\n",
        "\n",
        "  # Your code here to update the initial state with the weights from the\n",
        "  # existing model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CBaqLUojgRyB"
      },
      "source": [
        "As an alternative to federated evaluation, we can mix federated training and centralized evaluation. First, we will create a dataset for centralized evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FNiD1MDCgRyB",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "\n",
        "def data(client, source=train_data):\n",
        "  return preprocess(\n",
        "      source.create_tf_dataset_for_client(client)).take(5)\n",
        "\n",
        "clients = test_data.client_ids[:10]\n",
        "# We concatenate the test datasets for evaluation with Keras.\n",
        "test_dataset = functools.reduce(\n",
        "    lambda d1, d2: d1.concatenate(d2),\n",
        "    [data(client, test_data) for client in clients])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qbgyWD0xgRyD"
      },
      "source": [
        "Now let's write a function to evaluate our model using Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "26DXnuQZgRyD",
        "colab": {}
      },
      "source": [
        "def keras_evaluate(state, round_num):\n",
        "  keras_model = load_model(batch_size=BATCH_SIZE)\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[FlattenedCategoricalAccuracy()])\n",
        "  \n",
        "  # Your code here to take global model weights and push them back into a Keras\n",
        "  #  model to use its standard `.evaluate()` method.\n",
        "  \n",
        "\n",
        "  loss, accuracy = keras_model.evaluate(test_dataset, steps=2, verbose=0)\n",
        "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XT8fcNejgRyF"
      },
      "source": [
        "Federated learning can be useful in a variety of scenarios. In a cross-silo setting, there are a few clients which are highly available. In a cross-device setting, we are dealing with potentially a very large population of user devices, only a fraction of which may be available for training at a given point in time. This is the case, for example, when the client devices are mobile phones that participate in training only when plugged into a power source, off a metered network, and otherwise idle.\n",
        "\n",
        "Of course, we are in a simulation environment, and all the data is locally available. Typically then, when running simulations, we would simply sample a random subset of the clients to be involved in each round of training, generally different in each round.\n",
        "\n",
        "Try implementing sampling without replacement so that in each round, you train on 3 new clients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AmWMXqykgRyG",
        "colab": {}
      },
      "source": [
        "REPORT_GOAL = 3\n",
        "\n",
        "def sample_clients_without_replacement(remaining_clients):\n",
        "  # Your code here to return datasets for 3 randomly sampled clients."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x7cZQbPsgRyH"
      },
      "source": [
        "Finally, let's define a simple training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ja8cCWedgRyI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73160796-d4cb-40d6-a0db-8e6e7ff8795b"
      },
      "source": [
        "NUM_ROUNDS = 10\n",
        "remaining_clients = train_data.client_ids.copy()\n",
        "\n",
        "state = init_state()\n",
        "\n",
        "for round_num in range(NUM_ROUNDS):\n",
        "  print('Round {r}'.format(r=round_num))\n",
        "  keras_evaluate(state, round_num)\n",
        "  state, metrics = fed_avg.next(\n",
        "      state,\n",
        "      sample_clients_without_replacement(remaining_clients))\n",
        "  print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(\n",
        "      l=metrics['train']['loss'], a=metrics['train']['accuracy']))\n",
        "\n",
        "keras_evaluate(state, NUM_ROUNDS + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round 0\n",
            "\tEval: loss=3.539, accuracy=0.367\n",
            "Chose clients: ['THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_CONSTABLE'\n",
            " 'THE_TAMING_OF_THE_SHREW_MERCHANT'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_PETO']\n",
            "\n",
            "\tTrain: loss=4.325, accuracy=0.068\n",
            "Round 1\n",
            "\tEval: loss=4.304, accuracy=0.109\n",
            "Chose clients: ['THE_TRAGEDY_OF_KING_LEAR_SHYLOCK'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_CHANCELLOR'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_MELUN']\n",
            "\n",
            "\tTrain: loss=4.215, accuracy=0.144\n",
            "Round 2\n",
            "\tEval: loss=4.211, accuracy=0.195\n",
            "Chose clients: ['PERICLES__PRINCE_OF_TYRE_SON' 'THE_TRAGEDY_OF_KING_LEAR_BARNARDINE'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_LUCY']\n",
            "\n",
            "\tTrain: loss=4.132, accuracy=0.179\n",
            "Round 3\n",
            "\tEval: loss=4.056, accuracy=0.237\n",
            "Chose clients: ['ALL_S_WELL_THAT_ENDS_WELL_AUDREY' 'ALL_S_WELL_THAT_ENDS_WELL_SICINIUS'\n",
            " 'MUCH_ADO_ABOUT_NOTHING_SECOND_SENATOR']\n",
            "\n",
            "\tTrain: loss=4.080, accuracy=0.170\n",
            "Round 4\n",
            "\tEval: loss=3.977, accuracy=0.249\n",
            "Chose clients: ['ALL_S_WELL_THAT_ENDS_WELL_ADAM' 'ALL_S_WELL_THAT_ENDS_WELL_CANIDIUS'\n",
            " 'PERICLES__PRINCE_OF_TYRE_TYRREL']\n",
            "\n",
            "\tTrain: loss=3.976, accuracy=0.198\n",
            "Round 5\n",
            "\tEval: loss=3.926, accuracy=0.243\n",
            "Chose clients: ['PERICLES__PRINCE_OF_TYRE_SCROOP'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_SCROOP'\n",
            " 'ALL_S_WELL_THAT_ENDS_WELL_ROSALIND']\n",
            "\n",
            "\tTrain: loss=3.884, accuracy=0.189\n",
            "Round 6\n",
            "\tEval: loss=3.885, accuracy=0.201\n",
            "Chose clients: ['PERICLES__PRINCE_OF_TYRE_OXFORD'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_PUCELLE'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_FIRST_WARDER']\n",
            "\n",
            "\tTrain: loss=3.818, accuracy=0.193\n",
            "Round 7\n",
            "\tEval: loss=3.758, accuracy=0.248\n",
            "Chose clients: ['THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_PATIENCE'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_THIRD_WATCHMAN'\n",
            " 'THE_TRAGEDY_OF_KING_LEAR_FIRST_APPARITION']\n",
            "\n",
            "\tTrain: loss=0.000, accuracy=0.000\n",
            "Round 8\n",
            "\tEval: loss=3.829, accuracy=0.206\n",
            "Chose clients: ['THE_TRAGEDY_OF_KING_LEAR_BASSANIO'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_BURGUNDY'\n",
            " 'THE_TRAGEDY_OF_KING_LEAR_FIRST_FAIRY']\n",
            "\n",
            "\tTrain: loss=3.792, accuracy=0.174\n",
            "Round 9\n",
            "\tEval: loss=3.707, accuracy=0.248\n",
            "Chose clients: ['ALL_S_WELL_THAT_ENDS_WELL_CORIN'\n",
            " 'THE_FIRST_PART_OF_KING_HENRY_THE_FOURTH_RIVERS'\n",
            " 'ALL_S_WELL_THAT_ENDS_WELL_FIRST_OFFICER']\n",
            "\n",
            "\tTrain: loss=3.740, accuracy=0.186\n",
            "\tEval: loss=3.711, accuracy=0.229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "36huQmRVgRyK"
      },
      "source": [
        "## Experimenting with optimizers\n",
        "\n",
        "Using the tf.keras.optimizers API, you can experiment with many more client optimizers.\n",
        "\n",
        "For example, we could try using SGD with momentum on the clients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jk2NggPkgRyK",
        "colab": {}
      },
      "source": [
        "fed_avg = tff.learning.build_federated_averaging_process(\n",
        "    model_fn=create_tff_model,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=0.5, momentum=0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BNj3bMoRgRyM"
      },
      "source": [
        "On NLP datasets (such as Shakespeare), you may want to try an adaptive optimizer, such as Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gamBagw4gRyM",
        "colab": {}
      },
      "source": [
        "fed_avg = tff.learning.build_federated_averaging_process(\n",
        "    model_fn=create_tff_model,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(lr=0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0dThOWtbgRyP"
      },
      "source": [
        "As you can find out by studying the paper on the [Federated Averaging](https://arxiv.org/abs/1602.05629) algorithm, achieving convergence in a system with randomly sampled subsets of clients in each round can take a while, and it would be impractical to have to run hundreds of rounds in this interactive tutorial.\n",
        "\n",
        "However, if you train longer on more Shakespeare data, you should see a difference in the style of the text generated with the updated model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gllU1yzrgRyQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "2863683e-e2db-4cd1-d67a-268ef14cbd59"
      },
      "source": [
        "# Set our newly trained weights back in the originally created model.\n",
        "keras_model_batch1 = load_model(batch_size=1)\n",
        "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
        "# Text generation requires batch_size=1\n",
        "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What of TensorFlow Federated, you ask? Why so gracious\r\n",
            "crawlich in Suddenly to break into a large and glaring dread-expression, had reached the Father rode\r\n",
            "before him--knew him. \"Where it is, at Tellson's; it was\r\n",
            "prematurs\" said the ngr\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}